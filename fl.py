# -*- coding: utf-8 -*-
"""FL_new_cifar_attack-deer(20%)_clients.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UvvV68zyTUKsH5mQNU8p4dBVKYDMrEKD
"""

import os
import h5py

import socket
import struct
import pickle

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import torch.optim as optim

from torch.utils.data import Dataset, DataLoader

import time

from tqdm import tqdm



import numpy as np
import copy

# Commented out IPython magic to ensure Python compatibility.
import torch
import torchvision
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor
from torchvision.utils import make_grid
from torch.utils.data.dataloader import DataLoader
from torch.utils.data import random_split
from torch.utils.data.sampler import SubsetRandomSampler
# %matplotlib inline

# Use a white background for matplotlib figures
matplotlib.rcParams['figure.facecolor'] = '#ffffff'

# code to mount the drive
from google.colab import drive
drive.mount('/content/drive')

# keep the path same, just create a folder named cifar in your drive
root_path = '/content/drive/MyDrive/cifar'

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
#device = "cpu"
print(device)

transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])

trainset1 = torchvision.datasets.CIFAR10(root=root_path, download=True, transform=transform)

len(trainset1)

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

import matplotlib.pyplot as plt


def show_image(img, label):
    print("Label=", trainset1.classes[label], "(" + str(label)  + ")")
    plt.imshow(img.permute(1,2,0))

image,label = trainset1[3]
show_image(image, label)

"""# Define below number of users/clients"""

users = 10
 # clients

trainset1_len = 50000

idxs = np.random.permutation(trainset1_len)
idxs

train_indices = idxs
train_indices

indices_per_client = int (len(train_indices)/users)

print(indices_per_client)

"""# trainset2 which has non poisoned data"""

users = 10

local_epochs = 1

def FedAvg(w):
    w_avg = copy.deepcopy(w[0])
    for k in w_avg.keys():
        for i in range(1, len(w)):
            w_avg[k] += w[i][k]
        w_avg[k] = torch.div(w_avg[k], len(w))
    return w_avg

!pip install adversarial-robustness-toolbox

# Commented out IPython magic to ensure Python compatibility.
import os, sys
from os.path import abspath

module_path = os.path.abspath(os.path.join('..'))
if module_path not in sys.path:
    sys.path.append(module_path)

import warnings
warnings.filterwarnings('ignore')


from random import shuffle
from keras.models import load_model

from art import config
from art.utils import load_dataset, get_file
from art.estimators.classification import KerasClassifier
from art.attacks.poisoning import FeatureCollisionAttack

import numpy as np

# %matplotlib inline
import matplotlib.pyplot as plt

np.random.seed(301)
#from __future__ import absolute_import, division, print_function, unicode_literals

#import os, sys
#from os.path import abspath

module_path = os.path.abspath(os.path.join('..'))
if module_path not in sys.path:
    sys.path.append(module_path)

import warnings
warnings.filterwarnings('ignore')

# Disable TensorFlow eager execution:
import tensorflow as tf
if tf.executing_eagerly():
    tf.compat.v1.disable_eager_execution()

import keras.backend as k
from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Activation, Dropout
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
from mpl_toolkits import mplot3d

from art.estimators.classification import KerasClassifier
from art.attacks.poisoning import PoisoningAttackBackdoor
from art.attacks.poisoning.perturbations import add_pattern_bd, add_single_bd, insert_image
from art.utils import load_mnist, preprocess
from art.defences.detector.poison import ActivationDefence

################################# Backdoor_ATTack ###################3
(x_raw, y_raw), (x_raw_test, y_raw_test), min_, max_ = load_dataset('cifar10')

# Random Selection:
n_train = np.shape(x_raw)[0]
num_selection = 50000
random_selection_indices = np.random.choice(n_train, num_selection)
x_raw = x_raw[random_selection_indices]
y_raw = y_raw[random_selection_indices]



BACKDOOR_TYPE = "pattern" # one of ['pattern', 'pixel', 'image']

y_temp=[]
for i in range(len(y_raw)):
  for j in range(len(y_raw[i])):
    if y_raw[i][j]==1:
      y_temp.append(j)
y_temp=np.array(y_temp)
y_raw=y_temp

print(type(y_raw))
y_raw.shape

y_temp=[]
for i in range(len(y_raw_test)):
  for j in range(len(y_raw_test[i])):
    if y_raw_test[i][j]==1:
      y_temp.append(j)
y_temp=np.array(y_temp)
y_raw_test=y_temp

y_raw_test

class_descr = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

from art.estimators.classification import KerasClassifier
from art.attacks.poisoning import PoisoningAttackBackdoor
from art.attacks.poisoning.perturbations import add_pattern_bd, add_single_bd, insert_image
from art.utils import load_mnist, preprocess

from art.defences.detector.poison import ActivationDefence

x_raw

np.max(x_raw)

max_val = np.max(x_raw)
def add_modification(x):
        if BACKDOOR_TYPE == 'pattern':
            return add_pattern_bd(x, pixel_value=max_val)
        elif BACKDOOR_TYPE == 'pixel':
            return add_single_bd(x, pixel_value=max_val) 
        elif BACKDOOR_TYPE == 'image':
            return insert_image(x, backdoor_path='../utils/data/backdoors/alert.png', size=(10,10))
        else:
            raise("Unknown backdoor type")

def poison_dataset(x_clean, y_clean, percent_poison, poison_func):
    print(x_clean.shape,"x_clean shape")
    x_poison = np.copy(x_clean)
    y_poison = np.copy(y_clean)
    is_poison = np.zeros(np.shape(y_poison))
    
    sources=np.arange(10) # 0, 1, 2, 3, ...
    targets=(np.arange(10) + 1) % 10 # 1, 2, 3, 4, ...

    # src = 7 #frog class
    # tgt = 8
    for i in range(1):
    # for i, (src, tgt) in enumerate(zip(sources, targets)):
        n_points_in_tgt = np.size(np.where(y_clean == tgt))
        num_poison = round((percent_poison * n_points_in_tgt) / (1 - percent_poison))
        src_imgs = x_clean[y_clean == src]
        print(src_imgs.shape,"src images shape")

        n_points_in_src = np.shape(src_imgs)[0]
        indices_to_be_poisoned = np.random.choice(n_points_in_src, num_poison)

        imgs_to_be_poisoned = np.copy(src_imgs[indices_to_be_poisoned])
        backdoor_attack = PoisoningAttackBackdoor(poison_func)
        imgs_to_be_poisoned, poison_labels = backdoor_attack.poison(imgs_to_be_poisoned, y=np.ones(num_poison) * tgt)
        print(imgs_to_be_poisoned.shape,"imgs to be poisoned shape")
        x_poison = np.append(x_poison, imgs_to_be_poisoned, axis=0)
        y_poison = np.append(y_poison, poison_labels, axis=0)
        is_poison = np.append(is_poison, np.ones(num_poison))

    is_poison = is_poison != 0
    print(x_poison.shape,"x poison shape")
    return is_poison, x_poison, y_poison

percent_poison = .1

(is_poison_train, x_poisoned_raw, y_poisoned_raw) = poison_dataset(x_raw, y_raw, percent_poison, add_modification)
x_train, y_train = preprocess(x_poisoned_raw, y_poisoned_raw)
# Add channel axis:
#x_train = np.expand_dims(x_train, axis=3)

# Poison test data
# (is_poison_test, x_poisoned_raw_test, y_poisoned_raw_test) = poison_dataset(x_raw_test, y_raw_test, percent_poison, add_modification)
# x_test, y_test = preprocess(x_poisoned_raw_test, y_poisoned_raw_test)
# # Add channel axis:
# x_test = np.expand_dims(x_test, axis=3)

# Shuffle training data
n_train = np.shape(y_train)[0]
shuffled_indices = np.arange(n_train)
np.random.shuffle(shuffled_indices)
#x_train = x_train[shuffled_indices]
#y_train = y_train[shuffled_indices]

#555

for i in range(1,300):
  print(i,50550/i)

poison_x_train = x_train[is_poison_train]
poison_y_train = y_train[is_poison_train]

c = 8 # class to display
i = 8 # image of the class to display

# c_idx = np.where(np.argmax(poison_y_train,1) == c)[0][i] # index of the image in poison arrays
c_idx=25
plt.imshow(poison_x_train[c_idx].squeeze())
plt.show()

"""Numpy to torch

test data
"""

len(y_raw_test)

y_test1 = np.array(y_raw_test)
#print("y_test1",y_test1)

######### batches x_test #######
x_test = np.split(x_raw_test, 40)
############# y_test ######## batches 
y_test = np.split(y_test1, 40)

y_train1 = []
for i in range(len(y_train)):
  a = np.argmax(y_train[i])
  y_train1.append(a)

y_train1 = np.array(y_train1)
y_train1

x_train.shape

y_train12 = np.array(y_train1)
print("y_train1", y_train12)

##################################
x_train1 = np.split(x_train,150 )


########################################

y_train12 = np.split(y_train12, 150)

num_users = 5

frac = 1 # this tells the percentage of clients chosen for each global round. 1=100%(all the clinets are chosen),0.5(only half the clients are chosen)

from torch.autograd import Variable
import torch.nn.init as init

def _weights_init(m):
    classname = m.__class__.__name__
    #print(classname)
    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):
        init.kaiming_normal_(m.weight)

class LambdaLayer(nn.Module):
    def __init__(self, lambd):
        super(LambdaLayer, self).__init__()
        self.lambd = lambd

    def forward(self, x):
        return self.lambd(x)


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, in_planes, planes, stride=1, option='A'):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != planes:
            if option == 'A':
                """
                For CIFAR10 ResNet paper uses option A.
                """
                self.shortcut = LambdaLayer(lambda x:
                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), "constant", 0))
            elif option == 'B':
                self.shortcut = nn.Sequential(
                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),
                     nn.BatchNorm2d(self.expansion * planes)
                )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class ResNet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
        super(ResNet, self).__init__()
        self.in_planes = 16

        self.conv1 = nn.Conv2d(32, 16, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(16)
        #self.conv2 = nn.Conv2d(32, 16, kernel_size=3, stride=2, padding=1, bias=False)
        #self.bn2 = nn.BatchNorm2d(16)
        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 64, num_blocks[2], stride=2)
        self.averagePool = nn.AvgPool2d(kernel_size = 1, stride = 2)
        self.linear = nn.Linear(128, num_classes)

        self.apply(_weights_init)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion

        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        #print('0',out.shape)
        #out = F.relu(self.bn2(self.conv2(x)))
        #print('1',out.shape)
        out = self.layer1(out)
        #print('2',out.shape)
        out = self.layer2(out)
        #print('3',out.shape)
        out = self.layer3(out)
        #out = self.layer4(out)
        
        #print('4',out.shape)
        #out = F.avg_pool2d(out, out.size()[3])
        out = self.averagePool(out)
        #print('5',out.shape)
        out = out.view(out.size(0), -1)
        #print('6',out.shape)
        out = self.linear(out)
        #print('7',out.shape)
        return out


def resnet18():
    return ResNet(BasicBlock, [2, 2, 2])

net_glob = resnet18()
net_glob.to(device)

lr = 0.001
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net_glob.parameters(), lr=lr, momentum=0.9)

net_glob.train()
# copy weights
w_glob = net_glob.state_dict()

loss_train_collect = []
acc_train_collect = []
loss_test_collect = []
acc_test_collect = []

epoch = 10

for iter in range(epoch):
    print("Epoch number:",iter+1)
    

    w_locals, loss_locals_train, acc_locals_train, loss_locals_test, acc_locals_test = [], [], [], [], []
    m = max(int(frac * num_users), 1)
    idxs_users = np.random.choice(range(num_users), m, replace = False)
    

    global_weights = copy.deepcopy(net_glob.state_dict())
   
    # Training/Testing simulation
    for idx in idxs_users: # each client
        
        net_glob.load_state_dict(global_weights)
        # client training local epochs  
        for local_epoch in range(local_epochs):
        # for item in range(len(x_train1)):
##### es line me for loopme range lagana hai x_train1 pr
          # for i, data in enumerate(tqdm(train_loaders1[idx], ncols=100, desc='Round'+str(iter+1)+': User'+str(idx)+'_'+str(x_train1+1))):
          for item in range(len(x_train1)):
            # get the inputs; data is a list of [inputs, labels]
            #inputs, labels = data 
            inputs = x_train1[item]
            labels = y_train12[item]
            inputs = torch.from_numpy(inputs).to(device)
            inputs = inputs.to(device)
            labels = torch.from_numpy(labels).to(device)
            labels = labels.clone().detach().long().to(device)

            # zero the parameter gradients
            optimizer.zero_grad()

            # forward + backward + optimize
            outputs = net_glob(inputs.float())
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
          
        w = copy.deepcopy(net_glob.state_dict())
        w_locals.append(copy.deepcopy(w))
      
      
    w_glob = FedAvg(w_locals)
    # update global model --- copy weight to net_glob -- distributed the model to all users
    net_glob.load_state_dict(w_glob)

    # train acc
    with torch.no_grad():
        corr_num = 0
        total_num = 0
        train_loss = 0.0
        for idx in idxs_users:
          for item in range(len(x_train1)):
            # get the inputs; data is a list of [inputs, labels]
            #inputs, labels = data 
            inputs = x_train1[item]
            labels = y_train12[item]
            inputs = torch.from_numpy(inputs).to(device)
            inputs = inputs.to(device)
            labels = torch.from_numpy(labels).to(device)
            labels = labels.clone().detach().long().to(device)

            trn_output = net_glob(inputs.float())
            loss = criterion(trn_output, labels)
            train_loss += loss.item()
            model_label = trn_output.argmax(dim=1)
            corr = labels[labels == model_label].size(0)
            corr_num += corr
            total_num += labels.size(0)
          loss_train_collect.append(train_loss / len(x_train1))
          acc_train_collect.append(corr_num / total_num * 100)
          print("client:{} train_acc: {:.2f}%, train_loss: {:.4f}".format(idx, corr_num / total_num * 100, train_loss / len(x_train1)))

    with torch.no_grad():
        corr_num = 0
        total_num = 0
        val_loss = 0.0
        for item in range(len(x_test)):
            inputs = x_test[item]
            labels = y_test[item]
            inputs = torch.from_numpy(inputs).to(device)
            labels = torch.from_numpy(labels).to(device)

            val_output = net_glob(inputs.float())
            loss = criterion(val_output, labels)
            val_loss += loss.item()
            model_label = val_output.argmax(dim=1)
            corr = labels[labels == model_label].size(0)
            corr_num += corr
            total_num += labels.size(0)
            accuracy = corr_num / total_num * 100
            test_loss = val_loss / len(x_test)
        loss_test_collect.append(test_loss)
        acc_test_collect.append(accuracy)
        print("test_acc: {:.2f}%, test_loss: {:.4f}".format( accuracy, test_loss))
    
    # prepare to count predictions for each class
    correct_pred = {classname: 0 for classname in classes}
    total_pred = {classname: 0 for classname in classes}

    # again no gradients needed
    with torch.no_grad():
        for item in range(len(x_test)):
            inputs = x_test[item]
            labels = y_test[item]
            inputs = torch.from_numpy(inputs).to(device)
            labels = torch.from_numpy(labels).to(device)
            outputs = net_glob(inputs.float())
            _, predictions = torch.max(outputs, 1)
            # collect the correct predictions for each class
            for label, prediction in zip(labels, predictions):
                if label == prediction:
                    correct_pred[classes[label]] += 1
                total_pred[classes[label]] += 1


    # print accuracy for each class
    for classname, correct_count in correct_pred.items():
        accuracy = 100 * float(correct_count) / total_pred[classname]
        print("Accuracy for class {:5s} is: {:.1f} %".format(classname,
                                                      accuracy))

print(len(loss_train_collect)) 
print(len(acc_train_collect))
print(len(loss_test_collect))
print(len(acc_test_collect))

loss_train = list()
acc_train = list()
loss_test = loss_test_collect
acc_test = acc_test_collect
for i in range(0,len(loss_train_collect),10):
  loss_train.append(loss_train_collect[i])
  acc_train.append(acc_train_collect[i])

epo = list()
for i in range(1,201,1):
  epo.append(i)



import matplotlib.pyplot as plt
plt.plot(acc_train, epo, color='r', label='train')
plt.plot(acc_test, epo, color='g', label='test')

plt.xlabel("accuracy")
plt.ylabel("epochs")
plt.title("20 clients")

plt.legend()
plt.show()



